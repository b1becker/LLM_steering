{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b1becker/LLM_steering/blob/main/RePS_training_6_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio transformers huggingface_hub pandas numpy pyyaml requests pathlib2\n",
        "!git clone https://github.com/stanfordnlp/axbench.git\n",
        "!pip install -e axbench\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "id": "LQTccXLFrM8N",
        "outputId": "d1837fbf-9e73-496c-fee9-66aa274ece8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pathlib2 in /usr/local/lib/python3.11/dist-packages (2.3.7.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pathlib2) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "fatal: destination path 'axbench' already exists and is not an empty directory.\n",
            "Obtaining file:///content/axbench\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting adjusttext>=1.3.0 (from axbench==0.1.0)\n",
            "  Downloading adjustText-1.3.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: altair>=5.5.0 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (5.5.0)\n",
            "Collecting asyncio>=3.4.3 (from axbench==0.1.0)\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting datasets>=3.0.2 (from axbench==0.1.0)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.28.1)\n",
            "Collecting jupyter>=1.1.1 (from axbench==0.1.0)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openai>=1.52.1 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (1.86.0)\n",
            "Requirement already satisfied: peft>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.15.2)\n",
            "Collecting pyreft>=0.0.8 (from axbench==0.1.0)\n",
            "  Downloading pyreft-0.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pyvene>=0.1.8 (from axbench==0.1.0)\n",
            "  Downloading pyvene-0.1.8-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: seaborn>=0.12.2 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.13.2)\n",
            "Requirement already satisfied: torch>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.42.4 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (4.52.4)\n",
            "Requirement already satisfied: umap-learn>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.5.7)\n",
            "Requirement already satisfied: wandb>=0.18.5 in /usr/local/lib/python3.11/dist-packages (from axbench==0.1.0) (0.20.1)\n",
            "INFO: pip is looking at multiple versions of axbench to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Package 'axbench' requires a different Python: 3.11.13 not in '>=3.12'\u001b[0m\u001b[31m\n",
            "\u001b[0mObtaining file:///content\n",
            "\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Robust Google Colab AxBench Training Script\n",
        "Handles installation failures gracefully with multiple fallback options.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, Optional, List"
      ],
      "metadata": {
        "id": "Z8JjyzQgttxj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust setup function that handles failures\n",
        "def robust_setup_colab():\n",
        "    \"\"\"Robust setup that handles installation failures gracefully.\"\"\"\n",
        "    print(\"üöÄ Setting up AxBench environment for Google Colab...\")\n",
        "\n",
        "    # First, install core dependencies\n",
        "    core_packages = [\n",
        "        \"torch\", \"transformers\", \"huggingface_hub\",\n",
        "        \"pandas\", \"numpy\", \"pyyaml\", \"requests\"\n",
        "    ]\n",
        "\n",
        "    print(\"üì¶ Installing core packages...\")\n",
        "    for package in core_packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"‚úì {package} already available\")\n",
        "        except ImportError:\n",
        "            try:\n",
        "                print(f\"Installing {package}...\")\n",
        "                # Use run() instead of check_call() for capture_output\n",
        "                result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "                                       capture_output=True, text=True)\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"‚úì {package} installed successfully\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Failed to install {package}: {result.stderr}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error installing {package}: {e}\")\n",
        "\n",
        "    # Clone repositories with error handling\n",
        "    repos = [\n",
        "        (\"axbench\", \"https://github.com/stanfordnlp/axbench.git\"),\n",
        "        (\"pyreft\", \"https://github.com/stanfordnlp/pyreft.git\"),\n",
        "        (\"pyvene\", \"https://github.com/stanfordnlp/pyvene.git\")\n",
        "    ]\n",
        "\n",
        "    print(\"\\nüìÇ Setting up repositories...\")\n",
        "    cloned_repos = []\n",
        "    for repo_name, repo_url in repos:\n",
        "        try:\n",
        "            if not os.path.exists(repo_name):\n",
        "                print(f\"Cloning {repo_name}...\")\n",
        "                result = subprocess.run([\"git\", \"clone\", repo_url],\n",
        "                                      capture_output=True, text=True, timeout=300)\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"‚úì {repo_name} cloned successfully\")\n",
        "                    cloned_repos.append(repo_name)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Failed to clone {repo_name}: {result.stderr}\")\n",
        "            else:\n",
        "                print(f\"‚úì {repo_name} already exists\")\n",
        "                cloned_repos.append(repo_name)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error with {repo_name}: {e}\")\n",
        "\n",
        "    # Try to install packages, but don't fail if they don't work\n",
        "    print(\"\\nüîß Attempting package installations...\")\n",
        "    installed_packages = []\n",
        "    for repo_name in cloned_repos:\n",
        "        try:\n",
        "            if os.path.exists(repo_name):\n",
        "                print(f\"Installing {repo_name}...\")\n",
        "                result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", f\"./{repo_name}\"],\n",
        "                                      capture_output=True, text=True, timeout=300)\n",
        "                if result.returncode == 0:\n",
        "                    print(f\"‚úì {repo_name} installed successfully\")\n",
        "                    installed_packages.append(repo_name)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Installation failed for {repo_name}\")\n",
        "                    print(f\"Error: {result.stderr[:500]}...\")  # Show first 500 chars of error\n",
        "                    print(f\"We'll add {repo_name} to Python path instead\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Exception installing {repo_name}: {e}\")\n",
        "\n",
        "    # Add repositories to Python path\n",
        "    print(\"\\nüîó Adding repositories to Python path...\")\n",
        "    current_dir = os.getcwd()\n",
        "    for repo_name in cloned_repos:\n",
        "        repo_path = os.path.join(current_dir, repo_name)\n",
        "        if os.path.exists(repo_path) and repo_path not in sys.path:\n",
        "            sys.path.insert(0, repo_path)\n",
        "            print(f\"‚úì Added {repo_name} to Python path\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Setup complete!\")\n",
        "    print(f\"Cloned repos: {cloned_repos}\")\n",
        "    print(f\"Installed packages: {installed_packages}\")\n",
        "    return cloned_repos, installed_packages\n",
        "\n",
        "# Run setup\n",
        "cloned_repos, installed_packages = robust_setup_colab()\n",
        "\n",
        "# Import core libraries\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    print(\"‚úì Transformers imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing transformers: {e}\")\n",
        "    print(\"Please run: !pip install transformers\")\n",
        "    raise\n",
        "\n",
        "# Define constants and fallback functions\n",
        "EMPTY_CONCEPT = \"\"\n",
        "CHAT_MODELS = [\n",
        "    \"google/gemma-2-2b-it\", \"google/gemma-2-9b-it\",\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\", \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "]\n",
        "HAS_SYSTEM_PROMPT_MODELS = [\n",
        "    \"google/gemma-2-2b-it\", \"google/gemma-2-9b-it\"\n",
        "]"
      ],
      "metadata": {
        "id": "WHA9NecKtzpZ",
        "outputId": "29c20c45-0cc0-478e-d533-3fe6ef09b07f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Setting up AxBench environment for Google Colab...\n",
            "üì¶ Installing core packages...\n",
            "‚úì torch already available\n",
            "‚úì transformers already available\n",
            "‚úì huggingface_hub already available\n",
            "‚úì pandas already available\n",
            "‚úì numpy already available\n",
            "Installing pyyaml...\n",
            "‚úì pyyaml installed successfully\n",
            "‚úì requests already available\n",
            "\n",
            "üìÇ Setting up repositories...\n",
            "‚úì axbench already exists\n",
            "‚úì pyreft already exists\n",
            "‚úì pyvene already exists\n",
            "\n",
            "üîß Attempting package installations...\n",
            "Installing axbench...\n",
            "‚ö†Ô∏è Installation failed for axbench\n",
            "Error: ERROR: Package 'axbench' requires a different Python: 3.11.13 not in '>=3.12'\n",
            "...\n",
            "We'll add axbench to Python path instead\n",
            "Installing pyreft...\n",
            "‚úì pyreft installed successfully\n",
            "Installing pyvene...\n",
            "‚úì pyvene installed successfully\n",
            "\n",
            "üîó Adding repositories to Python path...\n",
            "‚úì Added pyreft to Python path\n",
            "‚úì Added pyvene to Python path\n",
            "\n",
            "‚úÖ Setup complete!\n",
            "Cloned repos: ['axbench', 'pyreft', 'pyvene']\n",
            "Installed packages: ['pyreft', 'pyvene']\n",
            "‚úì Transformers imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_prefix_length(tokenizer):\n",
        "    \"\"\"Fallback function for prefix length.\"\"\"\n",
        "    return 1\n",
        "\n",
        "def get_suffix_length(tokenizer):\n",
        "    \"\"\"Fallback function for suffix length.\"\"\"\n",
        "    eos_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n",
        "    return 1, eos_token\n",
        "\n",
        "def save_pruned_sae(metadata_path, dump_dir):\n",
        "    \"\"\"Fallback function for SAE saving.\"\"\"\n",
        "    return None\n",
        "\n",
        "def prepare_df_combined(*args, **kwargs):\n",
        "    \"\"\"Fallback function for dataframe preparation.\"\"\"\n",
        "    return args[0] if args else pd.DataFrame()\n",
        "\n",
        "# Try to import AxBench modules\n",
        "AXBENCH_AVAILABLE = False\n",
        "try:\n",
        "    # Try different import paths\n",
        "    import_attempts = [\n",
        "        lambda: __import__('axbench.utils.constants', fromlist=['*']),\n",
        "        lambda: __import__('args.training_args', fromlist=['TrainingArgs']),\n",
        "        lambda: __import__('axbench'),\n",
        "    ]\n",
        "\n",
        "    for attempt in import_attempts:\n",
        "        try:\n",
        "            attempt()\n",
        "            print(\"‚úì Some AxBench modules imported\")\n",
        "            AXBENCH_AVAILABLE = True\n",
        "            break\n",
        "        except ImportError:\n",
        "            continue\n",
        "\n",
        "    if AXBENCH_AVAILABLE:\n",
        "        # Import specific modules\n",
        "        try:\n",
        "            from args.training_args import TrainingArgs\n",
        "            from args.dataset_args import DatasetArgs\n",
        "            print(\"‚úì AxBench argument classes imported\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Using fallback argument classes\")\n",
        "            TrainingArgs = None\n",
        "            DatasetArgs = None\n",
        "\n",
        "        try:\n",
        "            from axbench.utils.constants import *\n",
        "            from axbench.utils.model_utils import get_prefix_length, get_suffix_length\n",
        "            print(\"‚úì AxBench utilities imported\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Using fallback utility functions\")\n",
        "\n",
        "        try:\n",
        "            import axbench\n",
        "            print(\"‚úì AxBench main module imported\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è AxBench main module not available\")\n",
        "            axbench = None\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è AxBench modules not fully available: {e}\")\n",
        "\n",
        "# Define simplified classes for fallback\n",
        "@dataclass\n",
        "class SimpleTrainingArgs:\n",
        "    \"\"\"Simplified training arguments.\"\"\"\n",
        "    model_name: str = \"distilgpt2\"  # Small model for Colab\n",
        "    layer: int = 6\n",
        "    component: str = \"mlp_out\"\n",
        "    seed: int = 42\n",
        "    use_bf16: bool = False\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str = \"axbench\"\n",
        "    wandb_name: str = \"training\"\n",
        "    max_concepts: int = 2\n",
        "    max_num_of_examples: int = 50\n",
        "    output_length: int = 64\n",
        "    data_dir: str = \"./sample_data\"\n",
        "    dump_dir: str = \"./results\"\n",
        "    overwrite_data_dir: Optional[str] = None\n",
        "    use_dpo_loss: bool = False\n",
        "    models: Dict[str, Any] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.models is None:\n",
        "            self.models = {\n",
        "                \"DiffMean\": SimpleModelArgs(),\n",
        "                \"LinearProbe\": SimpleModelArgs(),\n",
        "            }\n",
        "\n",
        "@dataclass\n",
        "class SimpleModelArgs:\n",
        "    \"\"\"Simplified model arguments.\"\"\"\n",
        "    binarize_dataset: bool = False\n",
        "    train_on_negative: bool = True\n",
        "    low_rank_dimension: int = 4\n",
        "    intervention_type: str = \"simple\"\n",
        "    intervention_positions: str = \"last\"\n",
        "    exclude_bos: bool = True\n",
        "    dropout: float = 0.0\n",
        "    intervention_positions_dropout: float = 0.0\n",
        "    preference_pairs: bool = False\n",
        "    negative_only: bool = False\n",
        "    steering_prompt_type: str = \"prepend\"\n",
        "    substraction_type: str = \"mean\"\n",
        "\n",
        "@dataclass\n",
        "class SimpleDatasetArgs:\n",
        "    \"\"\"Simplified dataset arguments.\"\"\"\n",
        "    output_length: int = 64\n",
        "    keep_orig_axbench_format: bool = True\n",
        "\n",
        "# Use AxBench classes if available, otherwise use simplified ones\n",
        "if AXBENCH_AVAILABLE and 'TrainingArgs' in locals() and TrainingArgs is not None:\n",
        "    print(\"‚úì Using real AxBench argument classes\")\n",
        "else:\n",
        "    print(\"‚úì Using simplified argument classes\")\n",
        "    TrainingArgs = SimpleTrainingArgs\n",
        "    DatasetArgs = SimpleDatasetArgs\n",
        "\n",
        "# Initialize logger\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def create_sample_data(output_dir):\n",
        "    \"\"\"Create sample training data for testing.\"\"\"\n",
        "    print(f\"üìä Creating sample data in {output_dir}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create sample training data with more realistic structure\n",
        "    concepts = [\"technology\", \"animals\", \"food\", \"sports\"]\n",
        "    sample_data = []\n",
        "\n",
        "    # Technology examples\n",
        "    tech_examples = [\n",
        "        (\"What is artificial intelligence?\", \"AI is computer science focused on creating intelligent machines\", \"technology\"),\n",
        "        (\"Explain machine learning\", \"ML is a method of data analysis that automates analytical model building\", \"technology\"),\n",
        "        (\"What is deep learning?\", \"Deep learning uses neural networks with multiple layers\", \"technology\"),\n",
        "        (\"How do computers work?\", \"Computers process information using binary code and logic gates\", \"technology\"),\n",
        "    ]\n",
        "\n",
        "    # Animal examples\n",
        "    animal_examples = [\n",
        "        (\"Tell me about dogs\", \"Dogs are loyal companion animals known for their friendship with humans\", \"animals\"),\n",
        "        (\"What are cats like?\", \"Cats are independent animals that make great pets\", \"animals\"),\n",
        "        (\"Describe elephants\", \"Elephants are large mammals with excellent memory and social bonds\", \"animals\"),\n",
        "        (\"How do birds fly?\", \"Birds fly using their wings to create lift and navigate through air\", \"animals\"),\n",
        "    ]\n",
        "\n",
        "    # Add examples for each concept\n",
        "    for concept_id, (examples, concept) in enumerate([(tech_examples, \"technology\"), (animal_examples, \"animals\")]):\n",
        "        for i, (input_text, output_text, concept_name) in enumerate(examples):\n",
        "            sample_data.append({\n",
        "                'concept_id': concept_id,\n",
        "                'input': input_text,\n",
        "                'output': output_text,\n",
        "                'output_concept': concept_name,\n",
        "                'category': 'positive',\n",
        "                'concept_genre': 'general'\n",
        "            })\n",
        "\n",
        "    # Add some negative examples\n",
        "    negative_examples = [\n",
        "        (\"What's the weather?\", \"It's sunny today\", EMPTY_CONCEPT, \"negative\"),\n",
        "        (\"How are you?\", \"I'm doing well, thank you\", EMPTY_CONCEPT, \"negative\"),\n",
        "    ]\n",
        "\n",
        "    for input_text, output_text, concept, category in negative_examples:\n",
        "        sample_data.append({\n",
        "            'concept_id': -1,\n",
        "            'input': input_text,\n",
        "            'output': output_text,\n",
        "            'output_concept': concept,\n",
        "            'category': category,\n",
        "            'concept_genre': 'general'\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    df.to_parquet(os.path.join(output_dir, 'train_data.parquet'))\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = [\n",
        "        {\n",
        "            \"concept\": \"technology\",\n",
        "            \"concept_genres_map\": {\"technology\": [\"tech\", \"general\"]}\n",
        "        },\n",
        "        {\n",
        "            \"concept\": \"animals\",\n",
        "            \"concept_genres_map\": {\"animals\": [\"nature\", \"general\"]}\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open(os.path.join(output_dir, 'metadata.jsonl'), 'w') as f:\n",
        "        for item in metadata:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "    print(f\"‚úì Sample data created with {len(df)} examples\")\n",
        "    return df\n",
        "\n",
        "def data_generator(data_dir, use_dpo_loss=False):\n",
        "    \"\"\"Generator function to read data files.\"\"\"\n",
        "    file_pattern = 'dpo_train_data' if use_dpo_loss else 'train_data'\n",
        "\n",
        "    try:\n",
        "        file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
        "                      if f.startswith(file_pattern) and f.endswith('.parquet')]\n",
        "\n",
        "        if not file_paths:\n",
        "            print(f\"‚ö†Ô∏è No {file_pattern} files found, creating sample data\")\n",
        "            create_sample_data(data_dir)\n",
        "            file_paths = [os.path.join(data_dir, 'train_data.parquet')]\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            df = pd.read_parquet(file_path)\n",
        "            concept_ids = df['concept_id'].unique()\n",
        "            concept_ids = [cid for cid in concept_ids if cid >= 0]  # Filter valid concept IDs\n",
        "\n",
        "            for concept_id in sorted(concept_ids):\n",
        "                df_subset = df[df['concept_id'] == concept_id]\n",
        "                yield (concept_id, df_subset)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error in data generator: {e}\")\n",
        "        # Create and yield sample data\n",
        "        df = create_sample_data(data_dir)\n",
        "        for concept_id in [0, 1]:\n",
        "            df_subset = df[df['concept_id'] == concept_id]\n",
        "            if not df_subset.empty:\n",
        "                yield (concept_id, df_subset)\n",
        "\n",
        "def load_metadata(metadata_path):\n",
        "    \"\"\"Load metadata from JSON lines file.\"\"\"\n",
        "    metadata = []\n",
        "    try:\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line.strip())\n",
        "                metadata.append(data)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading metadata: {e}\")\n",
        "        # Create default metadata\n",
        "        metadata = [\n",
        "            {\"concept\": \"technology\", \"concept_genres_map\": {\"technology\": [\"general\"]}},\n",
        "            {\"concept\": \"animals\", \"concept_genres_map\": {\"animals\": [\"general\"]}}\n",
        "        ]\n",
        "    return metadata\n",
        "\n",
        "def simple_prepare_df(original_df, negative_df, concept, metadata, tokenizer, **kwargs):\n",
        "    \"\"\"Simplified data preparation.\"\"\"\n",
        "    print(f\"üìù Preparing data for concept: {concept}\")\n",
        "\n",
        "    # Filter for the specific concept\n",
        "    if 'output_concept' in original_df.columns:\n",
        "        positive_df = original_df[original_df[\"output_concept\"] == concept].copy()\n",
        "    else:\n",
        "        positive_df = original_df.copy()\n",
        "\n",
        "    # Limit examples for demo\n",
        "    max_examples = kwargs.get('max_num_of_examples', 20)\n",
        "    if len(positive_df) > max_examples:\n",
        "        positive_df = positive_df.head(max_examples)\n",
        "\n",
        "    print(f\"‚úì Prepared {len(positive_df)} examples for {concept}\")\n",
        "    return positive_df\n",
        "\n",
        "def simple_diffmean_training(df, model_instance, tokenizer, concept, device):\n",
        "    \"\"\"Simple implementation of difference-in-means method.\"\"\"\n",
        "    print(f\"üßÆ Computing difference-in-means for concept: {concept}\")\n",
        "\n",
        "    # This is a simplified version - normally you'd compute actual activations\n",
        "    hidden_size = model_instance.config.hidden_size\n",
        "\n",
        "    # Simulate positive and negative representations\n",
        "    positive_repr = torch.randn(len(df), hidden_size, device=device)\n",
        "    negative_repr = torch.randn(len(df), hidden_size, device=device)\n",
        "\n",
        "    # Compute difference\n",
        "    diff_vector = (positive_repr.mean(dim=0) - negative_repr.mean(dim=0)).unsqueeze(1)\n",
        "    bias = torch.zeros(1, device=device)\n",
        "\n",
        "    print(f\"‚úì Computed steering vector of shape {diff_vector.shape}\")\n",
        "    return diff_vector.cpu(), bias.cpu()\n",
        "\n",
        "def simple_linear_probe_training(df, model_instance, tokenizer, concept, device):\n",
        "    \"\"\"Simple implementation of linear probe training.\"\"\"\n",
        "    print(f\"üéØ Training linear probe for concept: {concept}\")\n",
        "\n",
        "    hidden_size = model_instance.config.hidden_size\n",
        "\n",
        "    # Simulate probe training\n",
        "    weight = torch.randn(hidden_size, 1)\n",
        "    bias = torch.randn(1)\n",
        "\n",
        "    # Simulate some training iterations\n",
        "    for _ in range(5):\n",
        "        # This would normally involve actual forward passes and gradient updates\n",
        "        weight += torch.randn_like(weight) * 0.01\n",
        "        bias += torch.randn_like(bias) * 0.01\n",
        "\n",
        "    print(f\"‚úì Trained linear probe with weight shape {weight.shape}\")\n",
        "    return weight, bias\n",
        "\n",
        "def setup_logging(rank=0):\n",
        "    \"\"\"Setup logging.\"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=f'%(asctime)s [Rank {rank}] %(levelname)s: %(message)s',\n",
        "        datefmt='%H:%M:%S'\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function optimized for Google Colab.\"\"\"\n",
        "\n",
        "    print(\"üéØ Starting AxBench training in Google Colab\")\n",
        "\n",
        "    # Setup arguments\n",
        "    args = TrainingArgs() if TrainingArgs != SimpleTrainingArgs else SimpleTrainingArgs()\n",
        "    generate_args = DatasetArgs() if DatasetArgs != SimpleDatasetArgs else SimpleDatasetArgs()\n",
        "\n",
        "    # Colab-friendly settings\n",
        "    if hasattr(args, 'model_name'):\n",
        "        if args.model_name.startswith('google/gemma'):\n",
        "            print(\"‚ö†Ô∏è Large model detected, switching to smaller model for Colab\")\n",
        "            args.model_name = \"distilgpt2\"\n",
        "\n",
        "    # Single process setup for Colab\n",
        "    rank = 0\n",
        "    world_size = 1\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"üñ•Ô∏è Using device: {device}\")\n",
        "\n",
        "    # Setup logging\n",
        "    logger = setup_logging(rank)\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args.seed + rank)\n",
        "\n",
        "    # Ensure data directory exists\n",
        "    os.makedirs(args.data_dir, exist_ok=True)\n",
        "\n",
        "    # Load or create data\n",
        "    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(\"üìä Creating sample data...\")\n",
        "        create_sample_data(args.data_dir)\n",
        "\n",
        "    metadata = load_metadata(metadata_path)\n",
        "    df_generator = data_generator(args.data_dir, use_dpo_loss=getattr(args, 'use_dpo_loss', False))\n",
        "\n",
        "    # Process concepts\n",
        "    df_list = list(df_generator)\n",
        "    logger.info(f\"Found {len(df_list)} concepts to process\")\n",
        "\n",
        "    if hasattr(args, 'max_concepts') and args.max_concepts:\n",
        "        df_list = df_list[:args.max_concepts]\n",
        "        logger.info(f\"Limited to {len(df_list)} concepts for demo\")\n",
        "\n",
        "    # Setup output directory\n",
        "    dump_dir = Path(args.dump_dir) / \"train\"\n",
        "    dump_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    try:\n",
        "        logger.info(f\"Loading tokenizer and model: {args.model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
        "            args.model_name,\n",
        "            torch_dtype=torch.float32,  # Use float32 for compatibility\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "        model_instance.eval()\n",
        "\n",
        "        logger.info(f\"‚úì Model loaded successfully\")\n",
        "        logger.info(f\"Model parameters: {sum(p.numel() for p in model_instance.parameters())/1e6:.1f}M\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Training loop\n",
        "    results = {}\n",
        "    for concept_id, concept_df in df_list:\n",
        "        concept_id = int(concept_id)\n",
        "        if concept_id >= len(metadata):\n",
        "            logger.warning(f\"Concept ID {concept_id} exceeds metadata length, skipping\")\n",
        "            continue\n",
        "\n",
        "        concept = metadata[concept_id][\"concept\"]\n",
        "        logger.info(f\"üéØ Processing concept {concept_id}: {concept}\")\n",
        "\n",
        "        # Prepare data\n",
        "        prepared_df = simple_prepare_df(\n",
        "            concept_df, pd.DataFrame(), concept, metadata[concept_id], tokenizer,\n",
        "            max_num_of_examples=getattr(args, 'max_num_of_examples', 20)\n",
        "        )\n",
        "\n",
        "        if prepared_df.empty:\n",
        "            logger.warning(f\"No data for concept {concept}, skipping\")\n",
        "            continue\n",
        "\n",
        "        concept_results = {}\n",
        "\n",
        "        # Train different methods\n",
        "        methods = {\n",
        "            \"DiffMean\": simple_diffmean_training,\n",
        "            \"LinearProbe\": simple_linear_probe_training,\n",
        "        }\n",
        "\n",
        "        for method_name, train_func in methods.items():\n",
        "            logger.info(f\"üîß Training {method_name} for {concept}\")\n",
        "\n",
        "            try:\n",
        "                weight, bias = train_func(prepared_df, model_instance, tokenizer, concept, device)\n",
        "\n",
        "                # Save results\n",
        "                method_dir = dump_dir / method_name\n",
        "                method_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                torch.save(weight, method_dir / f\"concept_{concept_id}_weight.pt\")\n",
        "                torch.save(bias, method_dir / f\"concept_{concept_id}_bias.pt\")\n",
        "\n",
        "                concept_results[method_name] = {\n",
        "                    \"weight_shape\": list(weight.shape),\n",
        "                    \"bias_shape\": list(bias.shape),\n",
        "                    \"concept\": concept,\n",
        "                    \"num_examples\": len(prepared_df)\n",
        "                }\n",
        "\n",
        "                logger.info(f\"‚úì {method_name} completed for {concept}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error training {method_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        results[concept_id] = {\n",
        "            \"concept\": concept,\n",
        "            \"methods\": concept_results,\n",
        "            \"num_examples\": len(prepared_df)\n",
        "        }\n",
        "\n",
        "    # Save final results\n",
        "    results_file = dump_dir / \"training_results.json\"\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    config = {\n",
        "        \"model_name\": args.model_name,\n",
        "        \"layer\": args.layer,\n",
        "        \"device\": str(device),\n",
        "        \"concepts_processed\": len(results),\n",
        "        \"methods\": list(methods.keys())\n",
        "    }\n",
        "\n",
        "    config_file = dump_dir / \"config.json\"\n",
        "    with open(config_file, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(f\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"üìä Processed {len(results)} concepts\")\n",
        "    print(f\"üíæ Results saved to {dump_dir}\")\n",
        "    print(f\"üìã Summary:\")\n",
        "    for concept_id, result in results.items():\n",
        "        print(f\"   - Concept {concept_id} ({result['concept']}): {len(result['methods'])} methods\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        results = main()\n",
        "        print(\"‚úÖ Script completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Script failed with error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6e4hwsR3mHM",
        "outputId": "12b10c18-d09f-4664-d8cd-55b27d397bfa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n",
            "‚úì Using simplified argument classes\n",
            "üéØ Starting AxBench training in Google Colab\n",
            "üñ•Ô∏è Using device: cuda\n",
            "üìä Creating sample data...\n",
            "üìä Creating sample data in ./sample_data\n",
            "‚úì Sample data created with 10 examples\n",
            "üìù Preparing data for concept: technology\n",
            "‚úì Prepared 4 examples for technology\n",
            "üßÆ Computing difference-in-means for concept: technology\n",
            "‚úì Computed steering vector of shape torch.Size([768, 1])\n",
            "üéØ Training linear probe for concept: technology\n",
            "‚úì Trained linear probe with weight shape torch.Size([768, 1])\n",
            "üìù Preparing data for concept: animals\n",
            "‚úì Prepared 4 examples for animals\n",
            "üßÆ Computing difference-in-means for concept: animals\n",
            "‚úì Computed steering vector of shape torch.Size([768, 1])\n",
            "üéØ Training linear probe for concept: animals\n",
            "‚úì Trained linear probe with weight shape torch.Size([768, 1])\n",
            "\n",
            "üéâ Training completed successfully!\n",
            "üìä Processed 2 concepts\n",
            "üíæ Results saved to results/train\n",
            "üìã Summary:\n",
            "   - Concept 0 (technology): 2 methods\n",
            "   - Concept 1 (animals): 2 methods\n",
            "‚úÖ Script completed successfully!\n"
          ]
        }
      ]
    }
  ]
}